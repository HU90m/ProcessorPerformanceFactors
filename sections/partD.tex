% This part is 50 marks and should not exceed two pages including figures.

\subsection{Introduction \& Background}
% TODO: REFS

When a pipelined CPU comes across a branch instruction, it must normally
block the pipeline until the outcome of the branch is known, so that the
correct instructions are fetched. This however is slow and inefficient -
the earlier parts of the pipeline must sit around idle, while the later
ones figure out which instructions are next.

A simple solution to this would be to guess the outcome of the branch
instruction randomly, and assume the outcome until the assumption is
proven incorrect.
The pipeline would fill up with instructions that have a \(50\%\) chance
of being incorrect, and if they are, the contents of the pipeline
can simply be discarded.
This strategy would negate the branch penalty half of the time.

A more advanced approach is branch prediction.
This technique is used by CPUs to improve their guess as to the outcome of
a branch instruction.
Based on information already availiable, the CPU must make a decision about
the most likely outcome of an upcoming branch instruction. The decision can
then be used as an assumption to continue filling the pipe with instructions,
hopefully negating the branch penalty more than half of the time.

\subsection{Techniques}

\subsubsection{Bimodal Predictor}
% TODO: REFS

A bimodal predictor is very simple.
The predictor tags every branch instruction with a two bit counter.
Whenever that particular branch is taken, the counter is incremented,
and whenever it is not taken, the counter is decremented.
The counter does not overflow.
The predictor predicts that a branch will be taken if its counter
is \(10\) or \(11\), and it predicts that a branch will not be taken
if its counter is \(00\) or \(01\).

\subsubsection{TAGE Predictor}
% Refs:
%    \cite{Michaud2005} ppm-like-tag-based-predictor.pdf
%    \cite{Seznec2006}  a-case-for-tage.pdf
%    \cite{Seznec2007}  the-l-tage-branch-predictor.pdf
%
% All three are from the same two authors. The first introduces a
% similar predictor, the second introduces TAGE, and in the third,
% they collaborate and make a cute paper together :)

The TAGE predictor, or the \textbf{Ta}gged \textbf{Ge}ometric history
length branch predictor is a more advanced technique for branch
prediction.

This predictor relies on the assumption that if the same sequence of
branches happens repeatedly, then the result of the next branch will often
be the same.
Furthermore, this predictor relies on the assumption that the longer
the length of matching past branch history, the higher
the probability of matching future branches will be.~\cite{Seznec2006}

A series of tagged history predictors are used, each using a geometrically
increasing history length - i.e. of the form
\(L_i = \left\lceil \alpha^{i-1} \cdot L_0 \right\rceil\).
If any of the predictors recognize the current branch history,
then the prediction with the longest history is used.
Otherwise, a simpler `base' predictor is used, for example a Bimodal
predictor.~\cite{Seznec2007,Michaud2005}

A TAGE branch predictor is provided by \texttt{gem5}'s \texttt{TAGE}
branch predictor. % ToDO: find out implementation details and write

\subsubsection{Perceptron Predictor}

% Refs:
%   \cite{Jimenez2001} This paper presents the idea of perceptrons in general
%   \cite{Jimenez2014} This paper fleshes it out to multiperspective+tage
%
% Neural network
% Global shift register
% Simple to implement

Another approach to predicting branches is to try and use a
neural network to learn and predict whether or not a branch will
be taken.
The simplest, and therefore easiest neural network to implement
efficiently in hardware is a perceptron.
This neural network simply makes a prediction from the weighted
sum of a set of predictors. i.e. for predictors \(\{ x_1, \ldots , x_n \}\),
and learned weights \(\{ a_1, \ldots , a_n \}\), a prediction, \(\hat{y}\) could
be calculated as shown in Equation~\ref{eq:perceptron}.

\begin{equation}
    \hat{y} = \sum_{i=1}^{n} \left( a_i \cdot x_i \right)
    \label{eq:perceptron}
\end{equation}

This neural network can be applied to branch prediction by interpreting
\(\hat{y} > 0\) as meaning a branch is likely, and \(\hat{y} < 0\) as meaning
a branch is unlikely. 
Furthermore, a rough and very efficient form of gradient
descent can be applied to the network, by incrementing all weights whose predictors
have signs matching the actual value of \(y\), and decrementing all weights which
do not. 
A set of weights can be learned seperately for each branch 
instruction.~\cite{Jimenez2001}

In the simplest perceptron branch predictors, the predictor values of \(x_i\)
are a global shift register of previous branch results, where \(x_i = +1\) would
mean that the \(i\)th most recent branch was taken, and \(x_i = -1\) would
mean that the \(i\)th most recent branch was not taken.

More complex perceptron predictors, feed the perceptron many more
factors. For example a local shift register of branch outcomes for the specific
branch that is being predicted, previous branch addresses, and counters for
how many times a branch has been repeated.
Such predictors may be termed
Multiperspective perceptron branch predictors, as they examine multiple
perspectives of the current situation to conclude whether or not a branch
is likely.

Perceptron branch predictors are provided by \texttt{gem5}'s \texttt{MultiperspectivePerceptron8KB}
and \texttt{MultiperspectivePerceptron64KB} branch predictors.
 % ToDO: find out implementation details and write


\subsection{Simulation}

For this part of the coursework, the \texttt{gem5} simulator was invoked with the
command \texttt{
    .../gem5.opt~.../se.py
    -{}-bp-type=<bp>
    -{}-cpu-type=DerivO3CPU
    -{}-caches
    -{}-l1d\_size=16KB
    -{}-l1i\_size=16KB
    -c~<cmd>
    -o~<args>
}, for both benchmark commands, on both CPU architectures, with the branch
predictor types \texttt{BiModeBP}, \texttt{TAGE},
\texttt{MultiperspectivePerceptron8KB},
and~\texttt{MultiperspectivePerceptron64KB}.

\subsection{Results}

The results of our simulations in terms of CPI are shown in Figure~\ref{fig:partd-cpi}.

% OBSERVATIONS
%  Only SUSAN really changes, not CRC
%  arm crc has  133168908 branches
%  arm susan has 27942399 branches
%  x86 crc has  133165845 branches
%  x86 susan has 27114764 branches
%
%  CRC has far more branches but mispredict rate is almost always 0%
%  hence no improvement.
%
%  CPI and branch mispredict rates match up, which is good!
%
%  No consistent difference between X86 and ARM.
%
%  64KB perceptron is terrible (overfitting??)
%
% TODO: Look at all the actual implementations and see what is inside

\begin{figure}[b]%
\begin{subfigure}{.5\textwidth}
    \centering
    \includestandalone[width=\textwidth]{graphs/partd-cpi}
    \caption{CPI}
    \label{fig:partd-cpi}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
    \centering
    \includestandalone[width=\textwidth]{graphs/partd-mispredict}
    \caption{Mispredict rates (\%)}
    \label{fig:partd-mispredict}
\end{subfigure}%
\caption{A comparison of performance factors for CPUs with different branch predictors.}
\label{fig:partd-factors}
\end{figure}

