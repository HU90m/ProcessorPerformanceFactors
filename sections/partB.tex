% This part is 30 marks and should not exceed two pages including figures.

\subsection{Introduction \& Background}

The most basic form of cache associativity is Direct Mapped Cache, corresponding to an
associativity of 1.
In this case, each address in main memory is mapped to only one location in cache.
Because each data address doesn't have a choice in what cache cell it can be placed,
it can lead to situations where cache is overwritten when there are still spaces in
the cache that aren't being used.

A solution to this problem is to use set-associative cache.
In this system, a memory address is given a number of cache blocks it can be written to.
This number of cache blocks is called the associativity of the cache.

When loading a series of data from main memory to the cache, it may be the case that
multiple of these data requests are for data which exist consecutively in memory.
This situation revels an opportunity to speed up the program by not only loading
the data you are requesting into the cache, but also at the same time preemptively loading
the data located before and after the address of the data that the program just requested.

This can save time as it means some data will be preemptively loaded into the cache right
before it is needed.
However, it can also decrease performance if the program doesn't use data in the order that
it exists in memory, because time is wasted loading data into the cache which will go unused
by the processor.

\subsection{Method}

The \texttt{gem5} simulation was performed for only the X86 architecture, for each of the
SUSAN and CRC32 benchmark programs.
In every simulation, the size of the instruction cache was set to 8kB and the size
of the data cache was set to 16kB.
For each of these, the l1d\_assoc (data cache association) argument was iterated
through the set \{1, 2, 4, 8, 16, 32, 64\} and the cacheline\_size (cache line size) argument
was iterated through the set \{16kB, 32kB, 64kB, 128kB\}.

\begin{lstlisting}[
    language=beef,
    caption={The command used to invoke the \texttt{gem5} simulation for the SUSAN benchmark.}
]
./build/${architecture}/gem5.opt configs/example/se.py --l1d_size=16kB --l1i_size=8kB --caches --l1d_assoc=${assoc} --cacheline_size=${cacheline} --cpu-type=TimingSimpleCPU -c ../mibench/automotive/susan/susan -o "../mibench/automotive/susan/input_large.pgm output_large.smoothing.pgm -s"
\end{lstlisting}

\begin{lstlisting}[
    language=beef,
    caption={The command used to invoke the \texttt{gem5} simulation for the CRC32 benchmark.}
]
./build/${architecture}/gem5.opt configs/example/se.py --l1d_size=16kB --l1i_size=8kB --caches --l1d_assoc=${assoc} --cacheline_size=${cacheline} --cpu-type=TimingSimpleCPU -c ../mibench/telecomm/CRC32/crc -o "../mibench/telecomm/adpcm/data/large.pcm > output_large.txt"
\end{lstlisting}

This produced a dataset of 56 individual simulations with varying cache line sizes and
associativities.
Key values from the results of each of these simulations, such as number of cycles,
number of instructions, and miss rate were scraped from the output files and assembled 
in table format.

\subsection{Results \& Analysis}

\subsubsection{Cache Associativity}

From the results, it can be seen that increasing the associativity of the cache from 
1 to 2 greatly improves the miss rate of the processor. After that, increasing the 
associativity any further has little effect on the overall performance.

\begin{figure}[H]
    \centering
    \includestandalone[width=.6\textwidth]{graphs/partb-assoc}
    \caption{For different cache associativities}
    \label{fig:partb-assoc}
\end{figure}%

It can be seen that increasing the associativity can nearly completely eliminate
cache misses when running the CRC32 benchmark, however the SUSAN benchmark sees a near
constant 0.02\% miss rate from an associativity of 2 onwards.

\subsubsection{Cache Line Size}

The associativity of the cache makes a large difference on the effects of an increased cache
line size.

For an associativity of 1, it can be seen that increasing the cache line size from 16 to 32
improves performance for the SUSAN benchmark, but decreases performance for the CRC32
benchmark.
This shows that the SUSAN benchmark uses data more sequentially that the CRC32 benchmark
does.
This is expected because the SUSAN benchmark is used to do image analysis where it will be
looking at pixels of the image in the order they are laid out in the picture.
Cache line sizes of 64 and 128 only increase the cache miss rate further for
both benchmark programs.

For larger associativities, as cache line size increases, the performance increases.
This result is expected because with an associativity of 1 and a large cache line size, 
there is an increased chance of the processor overwriting actively used data in cache.
With a larger associativity however, this effect is negated as the cache is able to choose 
from many different cache blocks to write the data in.

\begin{figure}[H]%
\hfill%
\begin{subfigure}{.4\textwidth}
    \centering
    \includestandalone[width=\textwidth]{graphs/partb-2d1}
    \caption{Associativity=1}
    \label{fig:partb-2d1}
\end{subfigure}%
\hfill%
\begin{subfigure}{.4\textwidth}
    \centering
    \includestandalone[width=\textwidth]{graphs/partb-2d8}
    \caption{Associativity=8}
    \label{fig:partb-2d8}
\end{subfigure}%
\hfill\null%
\caption{The effects of cacheline size on miss rates, for an associativity of 1 and 8}
\end{figure}
